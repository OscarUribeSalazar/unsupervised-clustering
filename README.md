# üìåDetecci√≥n de Rese√±as Falsas 5 Estrellas Utilizando Modelos de Clasificaci√≥n No Supervisados

----
## üß© Resumen del Proyecto
Este proyecto tiene como objetivo identificar patrones y comportamientos asociados a rese√±as falsas en la plataforma de comercio electr√≥nico Mercado Libre. Para ello, se aplic√≥ una combinaci√≥n de t√©cnicas de procesamiento de texto, extracci√≥n de caracter√≠sticas, an√°lisis de correlaci√≥n, reducci√≥n de dimensionalidad (PCA) y algoritmos de agrupamiento como K-means y clustering jer√°rquico. Los datos fueron obtenidos usando la API de Mercado Libre, respetando sus restricciones de extracci√≥n.

Aunque los m√©todos no supervisados no lograron una separaci√≥n perfecta, se observaron agrupaciones con ciertas caracter√≠sticas comunes (e.g., rese√±as cortas con emojis y lenguaje positivo vs. rese√±as extensas y detalladas). Estos resultados permiten construir criterios preliminares para avanzar hacia un enfoque supervisado.

---
## üéØ Motivaci√≥n
Las rese√±as falsas afectan la confianza de los usuarios y distorsionan el posicionamiento de los productos. Los vendedores las utilizan para obtener ventaja competitiva, entregando productos gratuitos o reembolsos a cambio de rese√±as positivas. Esta pr√°ctica representa un problema √©tico, econ√≥mico y tecnol√≥gico que requiere herramientas automatizadas de detecci√≥n para mitigar su impacto.

---

## üóÉÔ∏è Descripci√≥n del Problema y los Datos
### ‚ö†Ô∏è Problem√°tica
Las plataformas como Mercado Libre no penalizan de manera efectiva estas pr√°cticas, lo que motiva su proliferaci√≥n. Los usuarios se ven expuestos a una percepci√≥n err√≥nea del producto. Por tanto, es crucial construir una m√©trica confiable que permita detectar la manipulaci√≥n en las rese√±as.

### üìä Descripci√≥n de los Datos
Se extrajeron m√°s de 226,000 productos y 172,000 rese√±as de 5 estrellas de la categor√≠a "Adornos y Decoraci√≥n del Hogar". El dataset incluye variables como n√∫mero de palabras, likes, dislikes, fecha de compra, n√∫mero de im√°genes, y presencia de palabras prohibidas.

---

## ‚öôÔ∏è Metodolog√≠a Aplicada
### üßπ Procesamiento inicial
* Eliminaci√≥n de variables innecesarias (reacciones, atributos, variacion_atributos).
* Eliminaci√≥n de rese√±as sin contenido.
* Eliminaci√≥n de rese√±as duplicadas.
* Transformaci√≥n de las fechas de compra y de creaci√≥n de la rese√±a.
* Creaci√≥n de la Variable de n√∫mero de caracteres que contiene una rese√±a.
* Creaci√≥n de la Variable de n√∫mero de d√≠as que un cliente tard√≥ en hacer la rese√±a desde que compr√≥ el producto.
* Selecciona variables de inter√©s.

### üîç An√°lisis Exploratorio
1. N√∫mero de rese√±as por valoraci√≥n
<img width="888" height="557" alt="image" src="https://github.com/user-attachments/assets/38ed89a8-7380-400a-9c42-4cf1a7d0bce8" /><p align="center"><em>N√∫mero de rese√±as por valoraci√≥n</em></p>
El 77% de las rese√±as de los productos analizados tienen una valoraci√≥n de 5 estrellas. 

2. An√°lisis de las variables m√°s relevantes
* N√∫mero de palabras por valoraci√≥n
<img width="810" height="538" alt="image" src="https://github.com/user-attachments/assets/a6cc37e6-99ea-44ed-b448-94cff693aa5e" /><p align="center"><em>N√∫mero de palabras por valoraci√≥n</em></p>
Las rese√±as con valoraciones de 1, 2 y 3 estrellas tienden a ser m√°s extensas en comparaci√≥n con las dem√°s valoraciones, esto puede ser debido a que los usuarios con malas experiencias tienden a expresarse m√°s que usuarios con buenas experiencias.

* D√≠as para crear una rese√±a
<img width="917" height="609" alt="image" src="https://github.com/user-attachments/assets/7a677f82-9383-4af6-84a0-3340a63fc9ca" /><p align="center"><em>Histograma de d√≠as para crear una rese√±a por valoraci√≥n</em></p>
El rango de d√≠as para crear una rese√±a oscila entre 0 y 1,000 d√≠as. Se puede apreciar una distribuci√≥n m√°s definida para los productos con una valoraci√≥n de 3 estrellas, sin embargo parece seguir una tendencia igualitaria para las dem√°s valoraciones.

* N√∫mero de caracteres por valoraci√≥n
<img width="852" height="566" alt="image" src="https://github.com/user-attachments/assets/2cfc1899-b6fb-4e8d-90e7-d23eeaed949d" /><p align="center"><em>Histograma de n√∫mero de caracteres por valoraci√≥n</em></p>
El n√∫mero de caracteres muestra una correlaci√≥n lineal similar, con rese√±as de 1, 2, 3 y 4 estrellas tendiendo a ser m√°s largas que las de 5 estrellas.


3. Procesamiento de Texto

Para el an√°lisis posterior, se seleccionaron √∫nicamente rese√±as con una valoraci√≥n de 5 estrellas. Esto se debe a que se asume que cada valoraci√≥n requiere un an√°lisis individual, ya que las tendencias de escritura pueden variar para cada una. 
El n√∫mero total de rese√±as de 5 estrellas dentro del dataset es de 172,310. Debido al poder de computaci√≥n limitado, se tom√≥ una muestra representativa aleatoria de 30,000 rese√±as con una valoraci√≥n de 5 estrellas y se aplic√≥ el siguiente procesamiento:
* Conversi√≥n de los documentos a tipo string.
* Eliminaci√≥n de saltos de l√≠nea (\n).
* Aplicaci√≥n de un autocorrector.
* Conversi√≥n a min√∫sculas.
* Eliminaci√≥n de stopwords.
* Eliminaci√≥n de caracteres especiales.
* Aplicaci√≥n de stemming.


4. Extracci√≥n de nuevas variables a partir del texto

Una vez procesados los documentos, se procedi√≥ a recopilar nuevas variables con el objetivo de obtener caracter√≠sticas adicionales de los datos. Las nuevas variables son:
* N√∫mero de oraciones por documento.
* N√∫mero de errores ortogr√°ficos por documento. 
* N√∫mero de palabras funcionales (preposiciones, conjunciones, determinantes, adverbios).
* Puntuaci√≥n de sentimiento por documento.
* Puntuaci√≥n de diccionario (dic score) por documento.
* N√∫mero de adjetivos utilizados.
* N√∫mero de sustantivos utilizados.
* N√∫mero de verbos utilizados.
* N√∫mero de entidades (organizaciones, lugares, fechas).
* N√∫mero de palabras √∫nicas.


5. An√°lisis de correlaci√≥n 

<img width="882" height="758" alt="image" src="https://github.com/user-attachments/assets/ec769894-b41e-479d-b045-eb411a419264" /><p align="center"><em>Matriz de correlaci√≥n</em></p>

Las variables calf, sustantivos, palabras_func, verbos, palabras_unicas y num_palabras presentan una correlaci√≥n lineal superior a 0.80 con otras variables. Por esta raz√≥n, se decidi√≥ eliminar estas variables para obtener una matriz con la mejor varianza explicada

6. Extracci√≥n de Vectores por documento

Con la intenci√≥n de capturar el contexto y la sintaxis de cada oraci√≥n, se aplic√≥ el siguiente procedimiento para la obtenci√≥n de vectores por cada documento:
* Ponderaci√≥n de palabra con TF-IDF: Esta ponderaci√≥n mide la frecuencia de una palabra en un conjunto de documentos y le asigna un valor en funci√≥n de dicha frecuencia. Palabras poco utilizadas tienden a tener un valor m√°s alto a comparaci√≥n de palabras m√°s utilizadas.
* Extracci√≥n de vectores por palabra utilizando el modelo Word2Vec: Esta librer√≠a extrae un vector de tama√±o ‚Äún‚Äù para cada palabra, bas√°ndose en las palabras que la rodean seg√∫n el n√∫mero de ventanas ‚Äún‚Äù utilizadas. Esto permite agrupar palabras en contextos similares.
* Ponderaci√≥n de vectores de Word2Vec con TF-IDF por documento: Utilizando la ponderaci√≥n de TF-IDF y los vectores obtenidos a partir de Word2Vec, se realiz√≥ una ponderaci√≥n por cada palabra utilizada en los documentos, obteniendo as√≠ un vector de documento de 100 dimensiones.
* Con los vectores documentos y las caracter√≠sticas obtenidas a partir del texto se unificaron ambas matrices para obtener una matriz conjunta de 118 caracter√≠sticas y 30,000 registros.

7. Aplicaci√≥n de PCA

<img width="946" height="473" alt="image" src="https://github.com/user-attachments/assets/503db5e9-bd11-4be9-aaaf-293985652714" /><p align="center"><em>Varianza Explicada PCA</em></p>

Esta gr√°fica indica que con 25 componentes principales se puede obtener el 76% de la varianza. Por lo tanto, la matriz para los an√°lisis posteriores tendr√° 25 caracter√≠sticas y 30,000 registros.

8. Aplicaci√≥n de algoritmos de agrupaci√≥n.

Posterior a la reducci√≥n de dimensionalidad se aplic√≥ una estandarizaci√≥n a los datos debido a las diferentes escalas que se presentan y se trabajaron con los dos siguientes modelos de agrupamiento:
* K-MENAS.
* Clustering Jer√°rquico.

---

## üìä An√°lisis de Resultados

### üß≠ PCA
<img width="754" height="598" alt="image" src="https://github.com/user-attachments/assets/1ae4e24d-ab9e-4106-bb98-ccf15ff659d9" /><p align="center"><em>Gr√°fico PCA con 0.76% de Varianza Explicada</em></p>

Aunque la matriz haya sido reducida en dimensiones mediante PCA, se puede observar una baja o casi nula separabilidad de los grupos. Esto se debe a que la extracci√≥n de caracter√≠sticas de los textos no ha logrado una separabilidad suficiente para permitir la formaci√≥n de grupos homog√©neos. Esta falta de separabilidad sugiere que las caracter√≠sticas seleccionadas no capturan adecuadamente las diferencias entre los grupos, lo que dificulta la identificaci√≥n de patrones claros y la correcta agrupaci√≥n de las rese√±as.

### üìå K-Means

<img width="946" height="473" alt="image" src="https://github.com/user-attachments/assets/1eca71f1-ba42-4eca-8b0d-9fdf69d7b79e" /><p align="center"><em>√çndices Elbow y Silhouette</em></p>

Si se aplica la metodolog√≠a de agrupaci√≥n K-MEANS utilizando los √≠ndices de "codo" y Silhouette, obtenemos valores bajos en su √≠ndice indicando una casi nula separabilidad de los datos. Esto se debe a que el m√©todo K-MEANS agrupa los puntos bas√°ndose en el "n" n√∫mero de centroides elegidos y asigna los puntos al centroide m√°s cercano

Es importante destacar que estamos buscando agrupaciones de datos dentro de una misma categor√≠a conforme a sus caracter√≠sticas. Es razonable pensar que existir√° una alta similitud entre estas caracter√≠sticas, lo que dificulta la diferenciaci√≥n si no se realiza una extracci√≥n de caracter√≠sticas adecuada.

Adem√°s, no buscamos √∫nicamente dos grupos que representen rese√±as falsas o verdaderas, sino varios subgrupos que puedan ser clasificados dentro de estas dos categor√≠as. Esta complejidad adicional hace a√∫n m√°s crucial la correcta extracci√≥n de caracter√≠sticas para identificar y separar adecuadamente los diferentes subgrupos.

Con la misma premisa de que en el conjunto de datos esperamos obtener m√°s de 2 agrupaciones diferentes y observamos que el √≠ndice de Silhouette al llegar a 14 hay una bajada significativa y se asignan estos n√∫mero de grupos al modelo de K-MEANS. Al realizar la asignaci√≥n de etiquetas obtenemos el siguiente gr√°fico:

<img width="940" height="483" alt="image" src="https://github.com/user-attachments/assets/19cb6e46-504c-446f-90d1-a95a00c328b3" /><p align="center"><em>Gr√°fico PCA con agrupaciones realizadas por K-Means</em></p>

Esto nos permite visualizar que el algoritmo de K-Means realiz√≥ una agrupaci√≥n en base al n√∫mero de centroides elegidos. Si visualizamos el tipo de rese√±as que se obtuvo con el n√∫mero de cluster elegidos podemos visualizar lo siguiente:

<img width="946" height="541" alt="image" src="https://github.com/user-attachments/assets/ea4c6508-fd96-4742-887d-51da95002039" /><p align="center"><em>Agrupaciones de rese√±as K-Means</em></p>

Aunque las clases se encuentran solapadas el m√©todo de K-MEANS logr√≥ una agrupaci√≥n en los datos conforme a sus caracter√≠sticas presentadas. Por ejemplo el el cluster n√∫mero 7 podemos observar que los documentos son cortos, utilizan un adjetivo y la utilizaci√≥n de ‚Äúemojis‚Äù, por otro lado el el cluster 4 podemos ver que son textos m√°s largos, que utilizan una correcta utilizaci√≥n de caracteres especiales, habla sobre las caracter√≠sticas del producto y mencionan detalles m√°s espec√≠ficos.

### üå≤ Clustering Jer√°rquico
<img width="660" height="523" alt="image" src="https://github.com/user-attachments/assets/c9815a3b-a8fe-4936-b544-2c02334082cc" /><p align="center"><em>Silhouette Score por diferentes n√∫meros de clusters</em></p>

Al visualizar el √≠ndice de Silhouette y, al igual que con K-means, podemos visualizar que el m√©todo no logr√≥ una separaci√≥n correcta. Aunque los √≠ndices de separaci√≥n son m√°s altos que los obtenidos con K-means.
Mayor estructura observada.

Con la misma premisa de que en el conjunto de datos esperamos obtener m√°s de 2 agrupaciones diferentes y deducimos que el n√∫mero adecuado de clusters es 5, ya que en ese punto se observa una bajada considerable del √≠ndice, obtenemos el siguiente gr√°fico con dos componentes principales. Mejores resultados en distribuci√≥n de cl√∫steres, especialmente en los extremos.

<img width="760" height="397" alt="image" src="https://github.com/user-attachments/assets/99829090-274d-4009-8c89-2f4206607a56" /><p align="center"><em>Gr√°fico PCA con agrupaciones realizadas por Clustering Jer√°rquico</em></p>

En este gr√°fico, observamos que una gran mayor√≠a de datos se concentran en el centro, mientras que las dem√°s agrupaciones comienzan a formarse a partir de los datos que se encuentran alejados de este centro.

Al observar algunas de las rese√±as conforme al cluster obtenido, notamos que los grupos fuera del centro logran tener caracter√≠sticas muy marcadas. Sin embargo, el m√©todo realiza un p√©simo trabajo al clasificar las clases centrales.

<img width="752" height="431" alt="image" src="https://github.com/user-attachments/assets/615fef6f-204c-4d1a-9a37-6202bab8e3c5" /><p align="center"><em>Agrupaciones de rese√±as Clustering Jer√°rquico</em></p>

Al visualizar el dendrograma obtenido por esta metodolog√≠a, podemos deducir que es posible obtener un n√∫mero mayor de clusters. 

<img width="940" height="527" alt="image" src="https://github.com/user-attachments/assets/c8b5ec85-280e-4661-87ba-4ac34a41f166" /><p align="center"><em>DendogramaClustering Jer√°rquico</em></p>

Al realizar una comparaci√≥n de ambos modelos pareciera ser que clustering jer√°rquico hace un mejor trabajo que k-means, esto puede ser debido a la metodolog√≠a que utiliza el modelo en empieza con cada data siendo un cluster e iterativamente va uniendo con otros datos.

Aunque los agrupamientos no lograron salir definidos claramente, se pueden extraer las caracter√≠sticas presentadas y realizar clasificaciones manuales a cada uno de los grupos para posteriormente trabajar con modelos supervisados.

---

## ‚úÖ Conclusiones

A trav√©s del uso de diversas t√©cnicas de procesamiento de texto y algoritmos de agrupamiento, se lograron identificar patrones importantes en las rese√±as. Sin embargo, para obtener resultados m√°s precisos y efectivos, es necesario realizar los siguientes pasos:

1.	**Definici√≥n Clara de Criterios:**** Con los clusters obtenidos a partir de las metodolog√≠as utilizadas, podemos empezar a definir criterios claros para identificar qu√© constituye una rese√±a falsa y una verdadera. Estos criterios permitir√°n analizar las agrupaciones y determinar cu√°les clases pueden ser encasilladas dentro de estos grupos. Una vez definidos, estos criterios facilitar√°n la transici√≥n a un enfoque supervisado, donde se podr√°n etiquetar manualmente las rese√±as como falsas o verdaderas para entrenar modelos de clasificaci√≥n.
2.	**Enfoque Supervisado:** El an√°lisis de texto generalmente se maneja mejor con un enfoque supervisado debido a la complejidad de extraer caracter√≠sticas significativas del texto. Con este enfoque, se pueden entrenar modelos de machine learning para aprender a identificar los contextos y caracter√≠sticas distintivas de las rese√±as falsas y verdaderas. Estos modelos supervisados permitir√°n una clasificaci√≥n m√°s precisa de nuevas entradas.
3.	**Profundizaci√≥n en el Procesamiento de Texto:** Es crucial explorar y aplicar t√©cnicas m√°s avanzadas de procesamiento de texto que consideren el contexto, la sintaxis y la sem√°ntica de las rese√±as. M√©todos como el an√°lisis de sentimientos m√°s detallado y la identificaci√≥n de entidades, pueden proporcionar una comprensi√≥n m√°s rica del contenido textual. Estas t√©cnicas avanzadas permitir√≠an una mejor agrupaci√≥n de las clases y una mayor eficiencia en la clasificaci√≥n.
4.	**Reducci√≥n de Sesgos:** El an√°lisis de texto est√° sujeto a varios posibles sesgos, que pueden surgir en diferentes etapas del procesamiento. Es esencial identificar y mitigar estos sesgos para garantizar la objetividad y precisi√≥n del an√°lisis. Esto incluye revisar y ajustar las t√©cnicas de preprocesamiento, como la tokenizaci√≥n, la eliminaci√≥n de stopwords y el stemming. Enfocar los esfuerzos en minimizar los sesgos ayudar√° a obtener resultados m√°s fiables y representativos.

En resumen, aunque se han logrado avances significativos en la identificaci√≥n de patrones mediante t√©cnicas de procesamiento de texto y agrupamiento, es evidente que se requiere un enfoque m√°s robusto y multifac√©tico para mejorar la precisi√≥n y efectividad de la clasificaci√≥n de rese√±as. Al seguir estos pasos, se espera desarrollar un sistema m√°s confiable y preciso para distinguir entre rese√±as falsas y verdaderas, mejorando as√≠ la transparencia y la confianza en las evaluaciones de productos en plataformas de comercio electr√≥nico.

---

üìö Referencias

* Mart√≠nez, L. (2018). An√°lisis de datos con Python.
* Flach, P. (2012). M√°quinas que aprenden.
* Camacho Collados, J., & Pilehvar, M. T. (2020). Embeddings in NLP.
* Jolliffe, I. T. (2002). Principal Component Analysis.


